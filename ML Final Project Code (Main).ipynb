{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724e5889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 18:59:38.756478: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 18:59:44.858928: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-24 18:59:44.860195: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-05-24 18:59:44.860207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "%load_ext tensorboard\n",
    "\n",
    "# %pip install kaggle\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5067e039",
   "metadata": {},
   "source": [
    "**Load Data in from Kaggle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f2a8e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from kaggle. Requires json credentials from kaggle.\n",
    "with open('kaggle.json') as f:\n",
    "    kaggle_json = json.load(f) \n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_json['username']\n",
    "os.environ['KAGGLE_KEY'] = kaggle_json['key']\n",
    "!kaggle datasets download iamsouravbanerjee/indian-food-images-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2d9952b-245c-4f6f-ade5-87bb5ed4b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip data\n",
    "from zipfile import ZipFile \n",
    "file_name = \"indian-food-images-dataset.zip\"\n",
    "with ZipFile(file_name, 'r') as zip: \n",
    "    print('Extracting all the files now...') \n",
    "    zip.extractall() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65c63c",
   "metadata": {},
   "source": [
    "**Set-up Training and Validation Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "861b03f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(rescale=1./255, # Scaling\n",
    "                                   shear_range=0.2, # Data Augmentation\n",
    "                                   zoom_range=0.2,\n",
    "                                   validation_split=0.2 # Validation set\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec0765f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9602 images belonging to 80 classes.\n",
      "Found 2387 images belonging to 80 classes.\n"
     ]
    }
   ],
   "source": [
    "training_data_path = \"Indian Food Images (All)/Indian Food Images\"\n",
    "\n",
    "target_size = (224,224)\n",
    "batch_size = 32\n",
    "\n",
    "train_set = datagen.flow_from_directory(\n",
    "    training_data_path,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    class_mode='categorical',\n",
    "    subset=\"training\",\n",
    "    seed = 24)\n",
    "validation_set = datagen.flow_from_directory(\n",
    "    training_data_path,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    class_mode='categorical',\n",
    "    subset=\"validation\",\n",
    "    seed = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3a6f65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: (224, 224, 3)\n",
      "Number of Classes: 80\n"
     ]
    }
   ],
   "source": [
    "image_shape = train_set.image_shape\n",
    "print(\"Image Shape:\", image_shape)\n",
    "\n",
    "num_classes = train_set.num_classes\n",
    "print(\"Number of Classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6fcc8a-b80a-4c55-a7eb-c1dc6e7be388",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0aaaf8a-e1f4-4c6d-b659-1816917ff656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet101, InceptionResNetV2, MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd489b9c-3438-4e5d-b698-eb38b788396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_on_top(base_model, dropout_rate=0.3, num_classes = num_classes):\n",
    "    inputs = base_model.input\n",
    "    x = Dense(512, activation='relu')(base_model.output)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9681dabf-4bee-4667-9987-aeeaa2dca657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "tensorboard_resnet101 = TensorBoard(log_dir='logs/ResNet101/')\n",
    "tensorboard_inception = TensorBoard(log_dir='logs/InceptionResNetV2/')\n",
    "tensorboard_mobilenet = TensorBoard(log_dir='logs/MobileNetV2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3443fa-e29a-4f7e-8dee-33d25445e262",
   "metadata": {},
   "source": [
    "**ResNet101**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb2720ef-e21e-4db3-b66a-366033e6979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet101(weights='imagenet', include_top=False, input_shape=image_shape, pooling='max')\n",
    "resnet.trainable = False\n",
    "resnet_top = build_classifier_on_top(resnet)\n",
    "resnet_top.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_top.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e6a458f-b95b-48a0-8596-e821e1e69556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 15:46:18.615584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8200\n",
      "2023-05-24 15:46:24.493716: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x55ffaec2d660 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-24 15:46:24.493756: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2023-05-24 15:46:24.547064: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-05-24 15:46:25.258733: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301/301 [==============================] - 556s 2s/step - loss: 4.4315 - accuracy: 0.0102 - val_loss: 4.3736 - val_accuracy: 0.0201\n",
      "Epoch 2/50\n",
      "301/301 [==============================] - 201s 667ms/step - loss: 4.3701 - accuracy: 0.0140 - val_loss: 4.3735 - val_accuracy: 0.0168\n",
      "Epoch 3/50\n",
      "301/301 [==============================] - 203s 676ms/step - loss: 4.3530 - accuracy: 0.0180 - val_loss: 4.3146 - val_accuracy: 0.0235\n",
      "Epoch 4/50\n",
      "301/301 [==============================] - 204s 678ms/step - loss: 4.3346 - accuracy: 0.0204 - val_loss: 4.3076 - val_accuracy: 0.0239\n",
      "Epoch 5/50\n",
      "301/301 [==============================] - 203s 674ms/step - loss: 4.3218 - accuracy: 0.0177 - val_loss: 4.3208 - val_accuracy: 0.0205\n",
      "Epoch 6/50\n",
      "301/301 [==============================] - 202s 672ms/step - loss: 4.3113 - accuracy: 0.0205 - val_loss: 4.2938 - val_accuracy: 0.0264\n",
      "Epoch 7/50\n",
      "301/301 [==============================] - 202s 672ms/step - loss: 4.3006 - accuracy: 0.0234 - val_loss: 4.2702 - val_accuracy: 0.0310\n",
      "Epoch 8/50\n",
      "301/301 [==============================] - 203s 675ms/step - loss: 4.2931 - accuracy: 0.0229 - val_loss: 4.2689 - val_accuracy: 0.0289\n",
      "Epoch 9/50\n",
      "301/301 [==============================] - 201s 667ms/step - loss: 4.2871 - accuracy: 0.0238 - val_loss: 4.2698 - val_accuracy: 0.0264\n",
      "Epoch 10/50\n",
      "301/301 [==============================] - 204s 679ms/step - loss: 4.2808 - accuracy: 0.0252 - val_loss: 4.2470 - val_accuracy: 0.0318\n",
      "Epoch 11/50\n",
      "301/301 [==============================] - 203s 673ms/step - loss: 4.2726 - accuracy: 0.0268 - val_loss: 4.2493 - val_accuracy: 0.0327\n",
      "Epoch 12/50\n",
      "301/301 [==============================] - 202s 669ms/step - loss: 4.2626 - accuracy: 0.0301 - val_loss: 4.2233 - val_accuracy: 0.0331\n",
      "Epoch 13/50\n",
      "301/301 [==============================] - 202s 671ms/step - loss: 4.2563 - accuracy: 0.0277 - val_loss: 4.2441 - val_accuracy: 0.0239\n",
      "Epoch 14/50\n",
      "301/301 [==============================] - 202s 671ms/step - loss: 4.2466 - accuracy: 0.0267 - val_loss: 4.2142 - val_accuracy: 0.0323\n",
      "Epoch 15/50\n",
      "301/301 [==============================] - 201s 666ms/step - loss: 4.2428 - accuracy: 0.0275 - val_loss: 4.1998 - val_accuracy: 0.0339\n",
      "Epoch 16/50\n",
      "301/301 [==============================] - 201s 668ms/step - loss: 4.2411 - accuracy: 0.0302 - val_loss: 4.2268 - val_accuracy: 0.0381\n",
      "Epoch 17/50\n",
      "301/301 [==============================] - 200s 665ms/step - loss: 4.2364 - accuracy: 0.0329 - val_loss: 4.2229 - val_accuracy: 0.0331\n",
      "Epoch 18/50\n",
      "301/301 [==============================] - 200s 664ms/step - loss: 4.2156 - accuracy: 0.0320 - val_loss: 4.1696 - val_accuracy: 0.0369\n",
      "Epoch 19/50\n",
      "301/301 [==============================] - 199s 662ms/step - loss: 4.2164 - accuracy: 0.0332 - val_loss: 4.1985 - val_accuracy: 0.0369\n",
      "Epoch 20/50\n",
      "301/301 [==============================] - 200s 665ms/step - loss: 4.2128 - accuracy: 0.0324 - val_loss: 4.1941 - val_accuracy: 0.0402\n",
      "Epoch 21/50\n",
      "301/301 [==============================] - 200s 666ms/step - loss: 4.2062 - accuracy: 0.0335 - val_loss: 4.1938 - val_accuracy: 0.0352\n",
      "Epoch 22/50\n",
      "301/301 [==============================] - 203s 672ms/step - loss: 4.2010 - accuracy: 0.0357 - val_loss: 4.1968 - val_accuracy: 0.0406\n",
      "Epoch 23/50\n",
      "301/301 [==============================] - 201s 668ms/step - loss: 4.1970 - accuracy: 0.0377 - val_loss: 4.1898 - val_accuracy: 0.0373\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mobilenet_top' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m history_resnet101 \u001b[38;5;241m=\u001b[39m resnet_top\u001b[38;5;241m.\u001b[39mfit(train_set, validation_data\u001b[38;5;241m=\u001b[39mvalidation_set, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m      2\u001b[0m                           callbacks\u001b[38;5;241m=\u001b[39m[early_stopping, tensorboard_resnet101])\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmobilenet_top\u001b[49m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFoodResNet101_v1.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_history/history_resnet101\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(history_resnet101\u001b[38;5;241m.\u001b[39mhistory, file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mobilenet_top' is not defined"
     ]
    }
   ],
   "source": [
    "history_resnet101 = resnet_top.fit(train_set, validation_data=validation_set, epochs=50, batch_size=batch_size,\n",
    "                          callbacks=[early_stopping, tensorboard_resnet101])\n",
    "resnet_top.save(\"FoodResNet101_v1.h5\")\n",
    "with open('train_history/history_resnet101', 'wb') as file:\n",
    "    pickle.dump(history_resnet101.history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a47c646-da70-4515-bd40-d901cd91b4fc",
   "metadata": {},
   "source": [
    "**InceptionResNetV2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d28879ca-002a-4e3c-86bf-7a88f7b8a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_resnet = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=image_shape, pooling='max')\n",
    "inception_resnet.trainable = False\n",
    "inception_resnet_top = build_classifier_on_top(inception_resnet)\n",
    "inception_resnet_top.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# inception_resnet_top.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f035c076-92ef-45b8-8aea-9a14a77427c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "301/301 [==============================] - 578s 2s/step - loss: 4.4162 - accuracy: 0.0245 - val_loss: 4.1882 - val_accuracy: 0.0524\n",
      "Epoch 2/50\n",
      "301/301 [==============================] - 220s 731ms/step - loss: 4.0315 - accuracy: 0.0516 - val_loss: 3.6274 - val_accuracy: 0.1089\n",
      "Epoch 3/50\n",
      "301/301 [==============================] - 218s 726ms/step - loss: 3.7871 - accuracy: 0.0778 - val_loss: 3.4747 - val_accuracy: 0.1441\n",
      "Epoch 4/50\n",
      "301/301 [==============================] - 217s 721ms/step - loss: 3.6732 - accuracy: 0.0899 - val_loss: 3.3349 - val_accuracy: 0.1684\n",
      "Epoch 5/50\n",
      "301/301 [==============================] - 218s 723ms/step - loss: 3.6117 - accuracy: 0.1024 - val_loss: 3.2117 - val_accuracy: 0.1848\n",
      "Epoch 6/50\n",
      "301/301 [==============================] - 217s 722ms/step - loss: 3.5151 - accuracy: 0.1157 - val_loss: 3.2137 - val_accuracy: 0.1910\n",
      "Epoch 7/50\n",
      "301/301 [==============================] - 217s 721ms/step - loss: 3.4762 - accuracy: 0.1227 - val_loss: 3.0882 - val_accuracy: 0.2090\n",
      "Epoch 8/50\n",
      "301/301 [==============================] - 216s 717ms/step - loss: 3.4290 - accuracy: 0.1301 - val_loss: 3.1251 - val_accuracy: 0.2095\n",
      "Epoch 9/50\n",
      "301/301 [==============================] - 214s 712ms/step - loss: 3.4617 - accuracy: 0.1208 - val_loss: 3.1788 - val_accuracy: 0.1839\n",
      "Epoch 10/50\n",
      "301/301 [==============================] - 215s 714ms/step - loss: 3.4244 - accuracy: 0.1305 - val_loss: 3.0948 - val_accuracy: 0.2254\n",
      "Epoch 11/50\n",
      "301/301 [==============================] - 215s 715ms/step - loss: 3.3939 - accuracy: 0.1332 - val_loss: 3.0754 - val_accuracy: 0.2325\n",
      "Epoch 12/50\n",
      "301/301 [==============================] - 214s 712ms/step - loss: 3.3496 - accuracy: 0.1368 - val_loss: 2.9844 - val_accuracy: 0.2493\n",
      "Epoch 13/50\n",
      "301/301 [==============================] - 214s 711ms/step - loss: 3.3363 - accuracy: 0.1444 - val_loss: 3.0156 - val_accuracy: 0.2396\n",
      "Epoch 14/50\n",
      "301/301 [==============================] - 215s 712ms/step - loss: 3.2906 - accuracy: 0.1540 - val_loss: 2.9712 - val_accuracy: 0.2271\n",
      "Epoch 15/50\n",
      "301/301 [==============================] - 215s 714ms/step - loss: 3.2762 - accuracy: 0.1590 - val_loss: 3.0120 - val_accuracy: 0.2430\n",
      "Epoch 16/50\n",
      "301/301 [==============================] - 216s 718ms/step - loss: 3.2294 - accuracy: 0.1627 - val_loss: 2.9336 - val_accuracy: 0.2463\n",
      "Epoch 17/50\n",
      "301/301 [==============================] - 215s 713ms/step - loss: 3.2448 - accuracy: 0.1579 - val_loss: 2.9024 - val_accuracy: 0.2614\n",
      "Epoch 18/50\n",
      "301/301 [==============================] - 216s 716ms/step - loss: 3.2339 - accuracy: 0.1681 - val_loss: 2.8768 - val_accuracy: 0.2723\n",
      "Epoch 19/50\n",
      "301/301 [==============================] - 215s 714ms/step - loss: 3.2008 - accuracy: 0.1674 - val_loss: 2.9280 - val_accuracy: 0.2610\n",
      "Epoch 20/50\n",
      "301/301 [==============================] - 214s 710ms/step - loss: 3.1778 - accuracy: 0.1708 - val_loss: 2.9202 - val_accuracy: 0.2438\n",
      "Epoch 21/50\n",
      "301/301 [==============================] - 214s 711ms/step - loss: 3.1321 - accuracy: 0.1851 - val_loss: 2.8183 - val_accuracy: 0.2719\n",
      "Epoch 31/50\n",
      "301/301 [==============================] - 213s 707ms/step - loss: 3.0829 - accuracy: 0.1894 - val_loss: 2.8457 - val_accuracy: 0.2623\n",
      "Epoch 32/50\n",
      "301/301 [==============================] - 213s 709ms/step - loss: 3.0365 - accuracy: 0.1989 - val_loss: 2.7908 - val_accuracy: 0.2752\n",
      "Epoch 33/50\n",
      "301/301 [==============================] - 214s 711ms/step - loss: 3.0486 - accuracy: 0.1956 - val_loss: 2.7726 - val_accuracy: 0.2828\n"
     ]
    }
   ],
   "source": [
    "history_inception_resnet = inception_resnet_top.fit(train_set, validation_data=validation_set, epochs=50, batch_size=batch_size,\n",
    "                          callbacks=[early_stopping, tensorboard_inception])\n",
    "inception_resnet_top.save(\"InceptionResNetV2_v1.h5\")\n",
    "with open('train_history/history_inception_resnet', 'wb') as file:\n",
    "    pickle.dump(history_inception_resnet.history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741f06f-2775-44df-99a1-e3f92969a395",
   "metadata": {},
   "source": [
    "**MobileNetV2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3179811-63a7-48c6-bf1f-3048da84c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet = MobileNetV2(weights='imagenet', include_top=False, input_shape=image_shape, pooling='max')\n",
    "mobilenet.trainable = False\n",
    "mobilenet_top = build_classifier_on_top(mobilenet)\n",
    "mobilenet_top.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# mobilenet_top.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9eaf3a-6e88-4370-8a3e-b7abd9ccb854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "301/301 [==============================] - 215s 703ms/step - loss: 4.2494 - accuracy: 0.0392 - val_loss: 3.4993 - val_accuracy: 0.1362\n",
      "Epoch 2/50\n",
      "301/301 [==============================] - 209s 695ms/step - loss: 3.5477 - accuracy: 0.1102 - val_loss: 3.0887 - val_accuracy: 0.1956\n",
      "Epoch 3/50\n",
      "301/301 [==============================] - 209s 693ms/step - loss: 3.1934 - accuracy: 0.1794 - val_loss: 2.7699 - val_accuracy: 0.2614\n",
      "Epoch 4/50\n",
      "301/301 [==============================] - 207s 689ms/step - loss: 2.9958 - accuracy: 0.2134 - val_loss: 2.6422 - val_accuracy: 0.2811\n",
      "Epoch 5/50\n",
      "301/301 [==============================] - 208s 690ms/step - loss: 2.8932 - accuracy: 0.2419 - val_loss: 2.5431 - val_accuracy: 0.2970\n",
      "Epoch 6/50\n",
      "301/301 [==============================] - 207s 689ms/step - loss: 2.7767 - accuracy: 0.2650 - val_loss: 2.4989 - val_accuracy: 0.3364\n",
      "Epoch 7/50\n",
      "301/301 [==============================] - 209s 694ms/step - loss: 2.6982 - accuracy: 0.2798 - val_loss: 2.4301 - val_accuracy: 0.3498\n",
      "Epoch 8/50\n",
      "301/301 [==============================] - 207s 690ms/step - loss: 2.6376 - accuracy: 0.2967 - val_loss: 2.3222 - val_accuracy: 0.3699\n",
      "Epoch 9/50\n",
      "301/301 [==============================] - 210s 697ms/step - loss: 2.5663 - accuracy: 0.3144 - val_loss: 2.3695 - val_accuracy: 0.3661\n",
      "Epoch 10/50\n",
      "301/301 [==============================] - 209s 695ms/step - loss: 2.5134 - accuracy: 0.3205 - val_loss: 2.2630 - val_accuracy: 0.3867\n",
      "Epoch 11/50\n",
      "301/301 [==============================] - 209s 697ms/step - loss: 2.4591 - accuracy: 0.3377 - val_loss: 2.2219 - val_accuracy: 0.4009\n",
      "Epoch 12/50\n",
      "301/301 [==============================] - 208s 690ms/step - loss: 2.4416 - accuracy: 0.3389 - val_loss: 2.2971 - val_accuracy: 0.3875\n",
      "Epoch 13/50\n",
      "301/301 [==============================] - 207s 690ms/step - loss: 2.3759 - accuracy: 0.3565 - val_loss: 2.1853 - val_accuracy: 0.4068\n",
      "Epoch 14/50\n",
      "301/301 [==============================] - 209s 695ms/step - loss: 2.3466 - accuracy: 0.3572 - val_loss: 2.2264 - val_accuracy: 0.3955\n",
      "Epoch 15/50\n",
      "301/301 [==============================] - 211s 702ms/step - loss: 2.3299 - accuracy: 0.3701 - val_loss: 2.1935 - val_accuracy: 0.4026\n",
      "Epoch 16/50\n",
      "301/301 [==============================] - 210s 698ms/step - loss: 2.3131 - accuracy: 0.3716 - val_loss: 2.1767 - val_accuracy: 0.4152\n",
      "Epoch 17/50\n",
      "301/301 [==============================] - 210s 696ms/step - loss: 2.2833 - accuracy: 0.3890 - val_loss: 2.1401 - val_accuracy: 0.4181\n",
      "Epoch 18/50\n",
      "301/301 [==============================] - 209s 694ms/step - loss: 2.2400 - accuracy: 0.3860 - val_loss: 2.1348 - val_accuracy: 0.4336\n",
      "Epoch 19/50\n",
      "301/301 [==============================] - 210s 697ms/step - loss: 2.2618 - accuracy: 0.3876 - val_loss: 2.1222 - val_accuracy: 0.4273\n",
      "Epoch 20/50\n",
      "301/301 [==============================] - 209s 696ms/step - loss: 2.2075 - accuracy: 0.3958 - val_loss: 2.1491 - val_accuracy: 0.4248\n",
      "Epoch 21/50\n",
      "301/301 [==============================] - 211s 702ms/step - loss: 2.1879 - accuracy: 0.4039 - val_loss: 2.0906 - val_accuracy: 0.4449\n",
      "Epoch 22/50\n",
      "301/301 [==============================] - 209s 694ms/step - loss: 2.1704 - accuracy: 0.4086 - val_loss: 2.1393 - val_accuracy: 0.4344\n",
      "Epoch 23/50\n",
      "301/301 [==============================] - 210s 699ms/step - loss: 2.1544 - accuracy: 0.4123 - val_loss: 2.1173 - val_accuracy: 0.4177\n",
      "Epoch 24/50\n",
      "301/301 [==============================] - 209s 694ms/step - loss: 2.1372 - accuracy: 0.4153 - val_loss: 2.1171 - val_accuracy: 0.4403\n",
      "Epoch 25/50\n",
      "301/301 [==============================] - 210s 698ms/step - loss: 2.1149 - accuracy: 0.4248 - val_loss: 2.0780 - val_accuracy: 0.4478\n",
      "Epoch 26/50\n",
      "301/301 [==============================] - 209s 693ms/step - loss: 2.1049 - accuracy: 0.4176 - val_loss: 2.1685 - val_accuracy: 0.4114\n",
      "Epoch 27/50\n",
      "301/301 [==============================] - 211s 702ms/step - loss: 2.1194 - accuracy: 0.4195 - val_loss: 2.1303 - val_accuracy: 0.4357\n",
      "Epoch 28/50\n",
      "301/301 [==============================] - 209s 695ms/step - loss: 2.0771 - accuracy: 0.4266 - val_loss: 2.1011 - val_accuracy: 0.4370\n",
      "Epoch 29/50\n",
      "301/301 [==============================] - 211s 701ms/step - loss: 2.0698 - accuracy: 0.4257 - val_loss: 2.0595 - val_accuracy: 0.4525\n",
      "Epoch 30/50\n",
      "301/301 [==============================] - 208s 692ms/step - loss: 2.0789 - accuracy: 0.4329 - val_loss: 2.0922 - val_accuracy: 0.4395\n",
      "Epoch 31/50\n",
      "301/301 [==============================] - 210s 697ms/step - loss: 2.0231 - accuracy: 0.4436 - val_loss: 2.1517 - val_accuracy: 0.4328\n",
      "Epoch 34/50\n",
      "301/301 [==============================] - 209s 694ms/step - loss: 2.0377 - accuracy: 0.4387 - val_loss: 2.0695 - val_accuracy: 0.4441\n"
     ]
    }
   ],
   "source": [
    "history_mobilenet = mobilenet_top.fit(train_set, validation_data=validation_set, epochs=50, batch_size=batch_size,\n",
    "                          callbacks=[early_stopping, tensorboard_mobilenet])\n",
    "mobilenet_top.save(\"MobileNetV2_v1.h5\")\n",
    "with open('train_history/history_mobilenet', 'wb') as file:\n",
    "    pickle.dump(history_mobilenet.history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5725e9-af3b-4902-a1c2-96ec3b6ba30f",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce8f8e52-594b-43bc-a84f-d7e2f7270b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_top = keras.models.load_model(\"MobileNetV2_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3378141a-6a56-4056-a04f-1a2a1ca042d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_top.trainable = True\n",
    "mobilenet_top.compile(optimizer=keras.optimizers.Adam(1e-4),  # Very low learning rate\n",
    "               loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fbea389-6bf7-40f3-bee9-adf57ff59bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobilenet_top.summary() # check layers are unfreezed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dc52a41-c660-465f-9ec2-81f752602192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "301/301 [==============================] - 240s 712ms/step - loss: 2.7908 - accuracy: 0.3090 - val_loss: 2.4025 - val_accuracy: 0.3695\n",
      "Epoch 2/100\n",
      "301/301 [==============================] - 210s 698ms/step - loss: 2.3279 - accuracy: 0.3796 - val_loss: 2.4631 - val_accuracy: 0.3532\n",
      "Epoch 3/100\n",
      "301/301 [==============================] - 211s 699ms/step - loss: 2.1745 - accuracy: 0.4152 - val_loss: 2.1301 - val_accuracy: 0.4344\n",
      "Epoch 4/100\n",
      "301/301 [==============================] - 210s 696ms/step - loss: 2.0826 - accuracy: 0.4363 - val_loss: 2.1066 - val_accuracy: 0.4437\n",
      "Epoch 5/100\n",
      "301/301 [==============================] - 209s 694ms/step - loss: 1.9559 - accuracy: 0.4653 - val_loss: 2.0153 - val_accuracy: 0.4667\n",
      "Epoch 6/100\n",
      "301/301 [==============================] - 212s 705ms/step - loss: 1.8806 - accuracy: 0.4817 - val_loss: 1.9108 - val_accuracy: 0.4881\n",
      "Epoch 7/100\n",
      "301/301 [==============================] - 208s 692ms/step - loss: 1.8259 - accuracy: 0.4932 - val_loss: 1.9019 - val_accuracy: 0.4973\n",
      "Epoch 8/100\n",
      "301/301 [==============================] - 210s 696ms/step - loss: 1.7438 - accuracy: 0.5152 - val_loss: 1.8347 - val_accuracy: 0.5082\n",
      "Epoch 9/100\n",
      "301/301 [==============================] - 210s 698ms/step - loss: 1.6694 - accuracy: 0.5341 - val_loss: 1.7945 - val_accuracy: 0.5174\n",
      "Epoch 10/100\n",
      "301/301 [==============================] - 208s 691ms/step - loss: 1.5961 - accuracy: 0.5494 - val_loss: 1.8442 - val_accuracy: 0.5149\n",
      "Epoch 11/100\n",
      "301/301 [==============================] - 209s 694ms/step - loss: 1.5896 - accuracy: 0.5527 - val_loss: 1.7354 - val_accuracy: 0.5346\n",
      "Epoch 12/100\n",
      "301/301 [==============================] - 208s 690ms/step - loss: 1.5343 - accuracy: 0.5595 - val_loss: 1.7426 - val_accuracy: 0.5459\n",
      "Epoch 13/100\n",
      "301/301 [==============================] - 209s 694ms/step - loss: 1.5145 - accuracy: 0.5738 - val_loss: 1.8794 - val_accuracy: 0.4969\n",
      "Epoch 14/100\n",
      "301/301 [==============================] - 208s 690ms/step - loss: 1.4073 - accuracy: 0.5960 - val_loss: 1.7156 - val_accuracy: 0.5316\n",
      "Epoch 15/100\n",
      "301/301 [==============================] - 208s 690ms/step - loss: 1.3896 - accuracy: 0.6011 - val_loss: 1.6007 - val_accuracy: 0.5660\n",
      "Epoch 16/100\n",
      "301/301 [==============================] - 208s 692ms/step - loss: 1.3503 - accuracy: 0.6071 - val_loss: 1.6136 - val_accuracy: 0.5668\n",
      "Epoch 17/100\n",
      "301/301 [==============================] - 209s 695ms/step - loss: 1.3103 - accuracy: 0.6248 - val_loss: 1.7629 - val_accuracy: 0.5354\n",
      "Epoch 18/100\n",
      "301/301 [==============================] - 210s 697ms/step - loss: 1.2612 - accuracy: 0.6405 - val_loss: 1.6824 - val_accuracy: 0.5656\n",
      "Epoch 19/100\n",
      "301/301 [==============================] - 208s 690ms/step - loss: 1.2532 - accuracy: 0.6368 - val_loss: 1.8529 - val_accuracy: 0.5249\n",
      "Epoch 20/100\n",
      "301/301 [==============================] - 209s 694ms/step - loss: 1.2003 - accuracy: 0.6534 - val_loss: 1.7960 - val_accuracy: 0.5496\n",
      "Epoch 21/100\n",
      "301/301 [==============================] - 208s 692ms/step - loss: 1.1600 - accuracy: 0.6625 - val_loss: 1.6731 - val_accuracy: 0.5584\n",
      "Epoch 22/100\n",
      "301/301 [==============================] - 209s 695ms/step - loss: 1.1411 - accuracy: 0.6656 - val_loss: 1.6861 - val_accuracy: 0.5702\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "tensorboard_mobilenet_ft = TensorBoard(log_dir='logs/mobilenet_ft/')\n",
    "\n",
    "history_mobilenet_ft = mobilenet_top.fit(train_set,\n",
    "                                         validation_data=validation_set,\n",
    "                                         epochs=100,\n",
    "                                         batch_size=batch_size,\n",
    "                          callbacks=[early_stopping, tensorboard_mobilenet_ft])\n",
    "mobilenet_top.save(\"MobileNetV2_ft.h5\")\n",
    "with open('train_history/history_mobilenet_ft', 'wb') as file:\n",
    "    pickle.dump(history_mobilenet_ft.history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ff6a2",
   "metadata": {},
   "source": [
    "**Custom CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5449b327-df96-4671-9fc3-e12fc9995544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_422 (Conv2D)         (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 111, 111, 32)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_423 (Conv2D)         (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 54, 54, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_424 (Conv2D)         (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 26, 26, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_425 (Conv2D)         (None, 24, 24, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 12, 12, 256)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_426 (Conv2D)         (None, 10, 10, 512)       1180160   \n",
      "                                                                 \n",
      " max_pooling2d_20 (MaxPoolin  (None, 5, 5, 512)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 12800)             0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 512)               6554112   \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 80)                41040     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,163,728\n",
      "Trainable params: 8,163,728\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=image_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "# Flatten the feature maps\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(80, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a96e45cd-3fb0-4c72-b681-771bec2cfbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "301/301 [==============================] - 193s 631ms/step - loss: 4.3178 - accuracy: 0.0174 - val_loss: 4.1760 - val_accuracy: 0.0226\n",
      "Epoch 2/100\n",
      "301/301 [==============================] - 189s 628ms/step - loss: 4.1609 - accuracy: 0.0265 - val_loss: 4.0800 - val_accuracy: 0.0314\n",
      "Epoch 3/100\n",
      "301/301 [==============================] - 190s 632ms/step - loss: 4.0998 - accuracy: 0.0342 - val_loss: 4.0400 - val_accuracy: 0.0415\n",
      "Epoch 4/100\n",
      "301/301 [==============================] - 190s 633ms/step - loss: 4.0266 - accuracy: 0.0479 - val_loss: 4.0110 - val_accuracy: 0.0452\n",
      "Epoch 5/100\n",
      "301/301 [==============================] - 191s 636ms/step - loss: 3.9204 - accuracy: 0.0586 - val_loss: 3.8454 - val_accuracy: 0.0683\n",
      "Epoch 6/100\n",
      "301/301 [==============================] - 189s 630ms/step - loss: 3.8085 - accuracy: 0.0739 - val_loss: 3.6920 - val_accuracy: 0.0926\n",
      "Epoch 7/100\n",
      "301/301 [==============================] - 190s 630ms/step - loss: 3.6929 - accuracy: 0.0939 - val_loss: 3.6012 - val_accuracy: 0.0968\n",
      "Epoch 8/100\n",
      "301/301 [==============================] - 189s 627ms/step - loss: 3.5907 - accuracy: 0.1112 - val_loss: 3.5317 - val_accuracy: 0.1265\n",
      "Epoch 9/100\n",
      "301/301 [==============================] - 190s 631ms/step - loss: 3.4994 - accuracy: 0.1296 - val_loss: 3.5034 - val_accuracy: 0.1324\n",
      "Epoch 10/100\n",
      "301/301 [==============================] - 190s 630ms/step - loss: 3.3967 - accuracy: 0.1479 - val_loss: 3.4819 - val_accuracy: 0.1362\n",
      "Epoch 11/100\n",
      "301/301 [==============================] - 191s 634ms/step - loss: 3.2925 - accuracy: 0.1705 - val_loss: 3.4476 - val_accuracy: 0.1345\n",
      "Epoch 12/100\n",
      "301/301 [==============================] - 193s 643ms/step - loss: 3.1745 - accuracy: 0.1949 - val_loss: 3.3844 - val_accuracy: 0.1605\n",
      "Epoch 13/100\n",
      "301/301 [==============================] - 189s 630ms/step - loss: 3.0664 - accuracy: 0.2158 - val_loss: 3.3659 - val_accuracy: 0.1684\n",
      "Epoch 14/100\n",
      "301/301 [==============================] - 188s 625ms/step - loss: 2.9644 - accuracy: 0.2369 - val_loss: 3.3250 - val_accuracy: 0.1827\n",
      "Epoch 15/100\n",
      "301/301 [==============================] - 189s 628ms/step - loss: 2.8403 - accuracy: 0.2643 - val_loss: 3.3136 - val_accuracy: 0.1793\n",
      "Epoch 16/100\n",
      "301/301 [==============================] - 190s 630ms/step - loss: 2.7333 - accuracy: 0.2869 - val_loss: 3.3385 - val_accuracy: 0.1940\n",
      "Epoch 17/100\n",
      "301/301 [==============================] - 190s 632ms/step - loss: 2.6571 - accuracy: 0.3021 - val_loss: 3.2524 - val_accuracy: 0.2216\n",
      "Epoch 18/100\n",
      "301/301 [==============================] - 191s 633ms/step - loss: 2.5772 - accuracy: 0.3252 - val_loss: 3.3206 - val_accuracy: 0.2128\n",
      "Epoch 19/100\n",
      "301/301 [==============================] - 190s 632ms/step - loss: 2.4780 - accuracy: 0.3440 - val_loss: 3.2480 - val_accuracy: 0.2354\n",
      "Epoch 20/100\n",
      "301/301 [==============================] - 190s 629ms/step - loss: 2.3445 - accuracy: 0.3722 - val_loss: 3.2754 - val_accuracy: 0.2401\n",
      "Epoch 21/100\n",
      "301/301 [==============================] - 190s 632ms/step - loss: 2.2641 - accuracy: 0.3892 - val_loss: 3.3175 - val_accuracy: 0.2304\n",
      "Epoch 22/100\n",
      "301/301 [==============================] - 190s 633ms/step - loss: 2.2174 - accuracy: 0.4009 - val_loss: 3.3236 - val_accuracy: 0.2463\n",
      "Epoch 23/100\n",
      "301/301 [==============================] - 191s 635ms/step - loss: 2.1048 - accuracy: 0.4314 - val_loss: 3.3522 - val_accuracy: 0.2417\n",
      "Epoch 24/100\n",
      "301/301 [==============================] - 191s 634ms/step - loss: 2.0218 - accuracy: 0.4462 - val_loss: 3.3989 - val_accuracy: 0.2421\n",
      "Epoch 25/100\n",
      "301/301 [==============================] - 189s 628ms/step - loss: 1.9380 - accuracy: 0.4704 - val_loss: 3.3854 - val_accuracy: 0.2606\n",
      "Epoch 26/100\n",
      "301/301 [==============================] - 188s 625ms/step - loss: 1.8709 - accuracy: 0.4794 - val_loss: 3.4357 - val_accuracy: 0.2698\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "tensorboard_mycnn = TensorBoard(log_dir='logs/my_cnn/')\n",
    "my_cnn_history = model.fit(train_set,\n",
    "                           validation_data=validation_set,\n",
    "                           epochs=100,\n",
    "                           batch_size=batch_size,\n",
    "                          callbacks=[early_stopping, tensorboard_mycnn])\n",
    "model.save(\"IndianFoodNet_v1.h5\")\n",
    "with open('train_history/my_cnn', 'wb') as file:\n",
    "    pickle.dump(my_cnn_history.history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "232d083f-fbf2-4d1a-a652-94f99af6ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    print(\"Training Accuracy:\")\n",
    "    print(model.evaluate(train_set))\n",
    "    print(\"Validation Accuracy:\")\n",
    "    print(model.evaluate(validation_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a690aa09-d502-439b-9a96-7b758d8576d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:\n",
      "301/301 [==============================] - 163s 540ms/step - loss: 1.1105 - accuracy: 0.7320\n",
      "[1.110529899597168, 0.7320349812507629]\n",
      "Validation Accuracy:\n",
      "75/75 [==============================] - 28s 370ms/step - loss: 3.4585 - accuracy: 0.2698\n",
      "[3.458545684814453, 0.2697947323322296]\n"
     ]
    }
   ],
   "source": [
    "eval_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f38627f8-960e-4d28-a7a1-9ddf7dc90a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:\n",
      "301/301 [==============================] - 170s 556ms/step - loss: 4.1404 - accuracy: 0.0505\n",
      "[4.140432357788086, 0.050510309636592865]\n",
      "Validation Accuracy:\n",
      "75/75 [==============================] - 30s 402ms/step - loss: 4.1839 - accuracy: 0.0364\n",
      "[4.1839470863342285, 0.036447424441576004]\n"
     ]
    }
   ],
   "source": [
    "eval_model(keras.models.load_model(\"FoodResNet101_v1.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e50311aa-dc16-475c-8d24-8a0f491444f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:\n",
      "301/301 [==============================] - 172s 560ms/step - loss: 2.6784 - accuracy: 0.3091\n",
      "[2.6784048080444336, 0.30910226702690125]\n",
      "Validation Accuracy:\n",
      "75/75 [==============================] - 30s 395ms/step - loss: 2.7630 - accuracy: 0.2866\n",
      "[2.7629549503326416, 0.28655216097831726]\n"
     ]
    }
   ],
   "source": [
    "eval_model(keras.models.load_model(\"InceptionResNetV2_v1.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb7ab265-26ce-441d-977e-e4d2f19083e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:\n",
      "301/301 [==============================] - 163s 539ms/step - loss: 1.4870 - accuracy: 0.5891\n",
      "[1.4870275259017944, 0.5891481041908264]\n",
      "Validation Accuracy:\n",
      "75/75 [==============================] - 28s 376ms/step - loss: 2.0938 - accuracy: 0.4378\n",
      "[2.0937535762786865, 0.4377880096435547]\n"
     ]
    }
   ],
   "source": [
    "eval_model(keras.models.load_model(\"MobileNetV2_v1.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7fd7a26-62fc-42d8-a6d6-be30c1e07509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:\n",
      "301/301 [==============================] - 164s 541ms/step - loss: 0.7375 - accuracy: 0.7920\n",
      "[0.7375173568725586, 0.7920224666595459]\n",
      "Validation Accuracy:\n",
      "75/75 [==============================] - 29s 381ms/step - loss: 1.6677 - accuracy: 0.5710\n",
      "[1.667717695236206, 0.571009635925293]\n"
     ]
    }
   ],
   "source": [
    "eval_model(keras.models.load_model(\"MobileNetV2_ft.h5\"))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
